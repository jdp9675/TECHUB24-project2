#!/usr/bin/env python3
"""
Advanced Travel Destination Recommendation Scraper
Scrapes weather, attractions, costs, and popularity data for cities worldwide
Provides intelligent recommendations based on multiple factors
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import random
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
import logging
import re
from collections import defaultdict

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s,%(msecs)03d - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

@dataclass
class CityData:
    """Data structure for city information"""
    name: str
    country: str
    temperature: float
    precipitation: float
    attractions_scraped: int
    cost_level: str
    overall_score: float
    weather_score: float
    activities_score: float
    popularity_score: float
    description: str = ""
    best_time_to_visit: str = ""
    currency: str = ""
    language: str = ""
    
    def to_dict(self) -> Dict:
        return asdict(self)

class TravelDestinationScraper:
    """Advanced scraper for travel destination data"""
    
    def __init__(self):
        self.session = requests.Session()
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        # Popular destinations to analyze
        self.destinations = [
            ("Bali", "Indonesia"), ("Bangkok", "Thailand"), ("Paris", "France"),
            ("Tokyo", "Japan"), ("New York", "USA"), ("Sydney", "Australia"),
            ("Prague", "Czech Republic"), ("Barcelona", "Spain"), ("London", "UK"),
            ("Rome", "Italy"), ("Dubai", "UAE"), ("Singapore", "Singapore"),
            ("Amsterdam", "Netherlands"), ("Vienna", "Austria"), ("Lisbon", "Portugal")
        ]
        
        # Weather API (OpenWeatherMap - requires API key, using mock data for demo)
        self.weather_api_key = "your_api_key_here"
        
    def get_headers(self) -> Dict[str, str]:
        """Get randomized headers"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
    
    def scrape_weather_data(self, city: str, country: str) -> Tuple[float, float]:
        """
        Scrape weather data for a city
        In production, you'd use a weather API like OpenWeatherMap
        For demo purposes, generating realistic weather data
        """
        try:
            # Mock weather data based on geographic location and season
            weather_patterns = {
                "Bali": (28.0, 2.5), "Bangkok": (32.0, 1.8), "Paris": (15.0, 4.2),
                "Tokyo": (18.0, 3.1), "New York": (12.0, 3.8), "Sydney": (22.0, 2.9),
                "Prague": (8.0, 3.5), "Barcelona": (19.0, 2.1), "London": (11.0, 4.5),
                "Rome": (17.0, 2.8), "Dubai": (35.0, 0.5), "Singapore": (29.0, 5.2),
                "Amsterdam": (9.0, 4.1), "Vienna": (7.0, 3.3), "Lisbon": (18.0, 2.4)
            }
            
            if city in weather_patterns:
                base_temp, base_precip = weather_patterns[city]
                # Add some random variation
                temp = base_temp + random.uniform(-3, 3)
                precip = base_precip + random.uniform(-1, 1)
                return max(temp, -10), max(precip, 0)
            
            # Fallback for unknown cities
            return random.uniform(10, 30), random.uniform(1, 5)
            
        except Exception as e:
            logger.warning(f"Weather data unavailable for {city}: {e}")
            return 20.0, 3.0
    
    def scrape_attractions_count(self, city: str, country: str) -> int:
        """
        Scrape number of attractions from travel websites
        This would typically scrape from TripAdvisor, Booking.com, etc.
        """
        try:
            # Mock attraction counts based on city popularity
            attraction_counts = {
                "Bali": 250, "Bangkok": 180, "Paris": 450, "Tokyo": 320,
                "New York": 380, "Sydney": 190, "Prague": 160, "Barcelona": 220,
                "London": 420, "Rome": 350, "Dubai": 200, "Singapore": 170,
                "Amsterdam": 140, "Vienna": 130, "Lisbon": 110
            }
            
            base_count = attraction_counts.get(city, 100)
            # Add random variation
            return base_count + random.randint(-20, 30)
            
        except Exception as e:
            logger.warning(f"Could not get attraction count for {city}: {e}")
            return 0
    
    def determine_cost_level(self, city: str, country: str) -> str:
        """
        Determine cost level based on economic data
        Scale: 1-10 (1=very cheap, 10=very expensive)
        """
        try:
            cost_levels = {
                "Bali": 3, "Bangkok": 4, "Paris": 8, "Tokyo": 9,
                "New York": 9, "Sydney": 8, "Prague": 5, "Barcelona": 7,
                "London": 9, "Rome": 7, "Dubai": 8, "Singapore": 8,
                "Amsterdam": 8, "Vienna": 7, "Lisbon": 6
            }
            
            level = cost_levels.get(city, 5) + random.randint(-1, 1)
            level = max(1, min(10, level))  # Clamp to 1-10
            
            return f"{level}/10"
            
        except Exception as e:
            logger.warning(f"Could not determine cost level for {city}: {e}")
            return "5/10"
    
    def scrape_popularity_data(self, city: str, country: str) -> float:
        """
        Scrape popularity data from search trends and travel websites
        """
        try:
            # Mock popularity based on tourist arrivals and online searches
            popularity_scores = {
                "Bali": 85.0, "Bangkok": 78.0, "Paris": 95.0, "Tokyo": 88.0,
                "New York": 92.0, "Sydney": 75.0, "Prague": 68.0, "Barcelona": 82.0,
                "London": 90.0, "Rome": 87.0, "Dubai": 79.0, "Singapore": 73.0,
                "Amsterdam": 71.0, "Vienna": 65.0, "Lisbon": 62.0
            }
            
            base_score = popularity_scores.get(city, 60.0)
            return base_score + random.uniform(-5, 5)
            
        except Exception as e:
            logger.warning(f"Could not get popularity data for {city}: {e}")
            return 50.0
    
    def calculate_scores(self, city_data: Dict) -> Tuple[float, float, float, float]:
        """
        Calculate weather, activities, popularity, and overall scores
        """
        # Weather score (higher is better, consider temperature and low precipitation)
        temp = city_data['temperature']
        precip = city_data['precipitation']
        
        # Ideal temperature range: 20-28Â°C
        if 20 <= temp <= 28:
            temp_score = 100
        elif 15 <= temp < 20 or 28 < temp <= 32:
            temp_score = 80
        elif 10 <= temp < 15 or 32 < temp <= 35:
            temp_score = 60
        else:
            temp_score = 30
        
        # Lower precipitation is better (for tourism)
        if precip <= 2:
            precip_score = 100
        elif precip <= 4:
            precip_score = 70
        elif precip <= 6:
            precip_score = 40
        else:
            precip_score = 20
        
        weather_score = (temp_score + precip_score) / 2
        
        # Activities score based on attractions
        attractions = city_data['attractions_scraped']
        if attractions >= 300:
            activities_score = 100
        elif attractions >= 200:
            activities_score = 80
        elif attractions >= 100:
            activities_score = 60
        elif attractions >= 50:
            activities_score = 40
        else:
            activities_score = 20
        
        # Popularity score (already calculated)
        popularity_score = city_data['popularity_score']
        
        # Overall score (weighted average)
        overall_score = (weather_score * 0.3 + activities_score * 0.4 + popularity_score * 0.3)
        
        return weather_score, activities_score, popularity_score, overall_score
    
    def get_additional_info(self, city: str, country: str) -> Dict[str, str]:
        """Get additional city information"""
        city_info = {
            "Bali": {
                "description": "Tropical paradise with beaches, temples, and rich culture",
                "best_time": "April-October",
                "currency": "Indonesian Rupiah",
                "language": "Indonesian"
            },
            "Bangkok": {
                "description": "Vibrant capital with street food, temples, and nightlife",
                "best_time": "November-March",
                "currency": "Thai Baht",
                "language": "Thai"
            },
            "Paris": {
                "description": "City of lights with art, cuisine, and historic landmarks",
                "best_time": "April-June, September-November",
                "currency": "Euro",
                "language": "French"
            },
            "Tokyo": {
                "description": "Modern metropolis blending tradition with innovation",
                "best_time": "March-May, September-November",
                "currency": "Japanese Yen",
                "language": "Japanese"
            },
            "New York": {
                "description": "The city that never sleeps with world-class attractions",
                "best_time": "April-June, September-November",
                "currency": "US Dollar",
                "language": "English"
            }
        }
        
        return city_info.get(city, {
            "description": f"Beautiful destination in {country}",
            "best_time": "Year-round",
            "currency": "Local currency",
            "language": "Local language"
        })
    
    def scrape_city_data(self, city: str, country: str) -> CityData:
        """Scrape comprehensive data for a single city"""
        logger.info(f"Scraping data for {city}, {country}")
        
        try:
            # Gather all data
            temperature, precipitation = self.scrape_weather_data(city, country)
            attractions_count = self.scrape_attractions_count(city, country)
            cost_level = self.determine_cost_level(city, country)
            popularity = self.scrape_popularity_data(city, country)
            
            # Create temporary data dict for score calculation
            temp_data = {
                'temperature': temperature,
                'precipitation': precipitation,
                'attractions_scraped': attractions_count,
                'popularity_score': popularity
            }
            
            # Calculate scores
            weather_score, activities_score, popularity_score, overall_score = self.calculate_scores(temp_data)
            
            # Get additional info
            additional_info = self.get_additional_info(city, country)
            
            # Create city data object
            city_data = CityData(
                name=city,
                country=country,
                temperature=round(temperature, 1),
                precipitation=round(precipitation, 1),
                attractions_scraped=attractions_count,
                cost_level=cost_level,
                overall_score=round(overall_score, 1),
                weather_score=round(weather_score, 1),
                activities_score=round(activities_score, 1),
                popularity_score=round(popularity_score, 1),
                description=additional_info.get("description", ""),
                best_time_to_visit=additional_info.get("best_time", ""),
                currency=additional_info.get("currency", ""),
                language=additional_info.get("language", "")
            )
            
            # Respectful delay
            time.sleep(random.uniform(1, 2))
            
            return city_data
            
        except Exception as e:
            logger.error(f"Error scraping {city}, {country}: {e}")
            return None
    
    def scrape_all_destinations(self, reference_city: str = "Sydney") -> List[CityData]:
        """Scrape data for all destinations"""
        logger.info(f"Starting comprehensive destination analysis for {reference_city}")
        
        all_cities = []
        
        for city, country in self.destinations:
            city_data = self.scrape_city_data(city, country)
            if city_data:
                all_cities.append(city_data)
        
        # Sort by overall score (descending)
        all_cities.sort(key=lambda x: x.overall_score, reverse=True)
        
        return all_cities
    
    def print_destination_report(self, cities: List[CityData], reference_city: str = "Sydney"):
        """Print formatted destination report like your example"""
        logger.info(f"Successfully scraped {len([c for c in cities if c.attractions_scraped > 0])} attractions for {reference_city}")
        print(f"\nð Top {len(cities)} Destination Recommendations:\n" + "="*60)
        
        for i, city in enumerate(cities, 1):
            print(f"\n{i}. {city.name}, {city.country}")
            print(f"   ð¡ï¸  Temperature: {city.temperature}Â°C")
            print(f"   ð§ï¸  Precipitation: {city.precipitation}mm/day")
            print(f"   ðï¸  Attractions Scraped: {city.attractions_scraped}")
            print(f"   ð° Cost Level: {city.cost_level}")
            print(f"   â­ Overall Score: {city.overall_score}/100")
            print(f"   Weather: {city.weather_score} | Activities: {city.activities_score} | Popularity: {city.popularity_score}")
            
            if city.description:
                print(f"   ð {city.description}")
            if city.best_time_to_visit:
                print(f"   ð Best Time: {city.best_time_to_visit}")
        
        # Performance metrics
        total_attractions = sum(c.attractions_scraped for c in cities)
        avg_score = sum(c.overall_score for c in cities) / len(cities) if cities else 0
        
        print(f"\nð Performance Metrics:")
        print(f"   Total Attractions Scraped: {total_attractions}")
        print(f"   Average Destination Score: {avg_score:.1f}/100")
        print(f"   Analysis Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    def save_data(self, cities: List[CityData], format_type: str = "both"):
        """Save scraped data to files"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format_type in ["json", "both"]:
            # JSON format
            json_data = {
                "scrape_timestamp": timestamp,
                "total_destinations": len(cities),
                "destinations": [city.to_dict() for city in cities],
                "top_recommendation": cities[0].to_dict() if cities else None,
                "performance_metrics": {
                    "total_attractions": sum(c.attractions_scraped for c in cities),
                    "avg_score": sum(c.overall_score for c in cities) / len(cities) if cities else 0,
                    "cities_analyzed": len(cities)
                }
            }
            
            json_filename = f"travel_destinations_{timestamp}.json"
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            print(f"ð¾ Data saved to {json_filename}")
        
        if format_type in ["csv", "both"]:
            # CSV format for easy analysis
            try:
                import pandas as pd
                df = pd.DataFrame([city.to_dict() for city in cities])
                csv_filename = f"travel_destinations_{timestamp}.csv"
                df.to_csv(csv_filename, index=False, encoding='utf-8')
                print(f"ð CSV data saved to {csv_filename}")
            except ImportError:
                print("Install pandas for CSV export: pip install pandas")
    
    def get_recommendations_for_user(self, preferences: Dict) -> List[CityData]:
        """Get personalized recommendations based on user preferences"""
        all_cities = self.scrape_all_destinations()
        
        if not preferences:
            return all_cities[:8]
        
        # Filter and score based on preferences
        filtered_cities = []
        
        for city in all_cities:
            score_modifier = 0
            
            # Temperature preference
            if 'preferred_temp' in preferences:
                temp_diff = abs(city.temperature - preferences['preferred_temp'])
                if temp_diff <= 5:
                    score_modifier += 10
                elif temp_diff <= 10:
                    score_modifier += 5
            
            # Budget preference
            if 'budget_level' in preferences:
                cost_num = int(city.cost_level.split('/')[0])
                budget_pref = preferences['budget_level']
                if abs(cost_num - budget_pref) <= 2:
                    score_modifier += 10
            
            # Activity preference
            if 'min_attractions' in preferences:
                if city.attractions_scraped >= preferences['min_attractions']:
                    score_modifier += 15
            
            # Apply score modifier
            modified_score = city.overall_score + score_modifier
            city.overall_score = modified_score
            filtered_cities.append(city)
        
        # Re-sort by modified scores
        filtered_cities.sort(key=lambda x: x.overall_score, reverse=True)
        
        return filtered_cities[:8]

def main():
    """Main function with interactive options"""
    print("ð TRAVEL DESTINATION RECOMMENDATION SCRAPER")
    print("=" * 50)
    
    scraper = TravelDestinationScraper()
    
    while True:
        print("\nð OPTIONS:")
        print("1. ð Analyze All Destinations")
        print("2. ð¯ Get Personalized Recommendations")
        print("3. ð Quick Analysis (Top 5)")
        print("4. ð Custom City Analysis")
        print("5. â Exit")
        
        choice = input("\nð Choose option (1-5): ").strip()
        
        try:
            if choice == '1':
                print("\nð Analyzing all destinations...")
                cities = scraper.scrape_all_destinations()
                scraper.print_destination_report(cities)
                scraper.save_data(cities)
                
            elif choice == '2':
                print("\nð¯ Personalized Recommendations")
                print("Enter your preferences (press Enter to skip):")
                
                preferences = {}
                
                temp_input = input("Preferred temperature (Â°C): ").strip()
                if temp_input:
                    try:
                        preferences['preferred_temp'] = float(temp_input)
                    except ValueError:
                        pass
                
                budget_input = input("Budget level (1-10, 1=cheap): ").strip()
                if budget_input:
                    try:
                        preferences['budget_level'] = int(budget_input)
                    except ValueError:
                        pass
                
                attractions_input = input("Minimum attractions desired: ").strip()
                if attractions_input:
                    try:
                        preferences['min_attractions'] = int(attractions_input)
                    except ValueError:
                        pass
                
                cities = scraper.get_recommendations_for_user(preferences)
                scraper.print_destination_report(cities)
                
            elif choice == '3':
                print("\nð Quick Analysis - Top 5 Destinations")
                cities = scraper.scrape_all_destinations()[:5]
                scraper.print_destination_report(cities)
                
            elif choice == '4':
                print("\nð Custom City Analysis")
                city_input = input("Enter cities (format: City,Country separated by semicolons): ")
                
                if city_input.strip():
                    custom_destinations = []
                    for entry in city_input.split(';'):
                        if ',' in entry:
                            city, country = entry.strip().split(',', 1)
                            custom_destinations.append((city.strip(), country.strip()))
                    
                    if custom_destinations:
                        scraper.destinations = custom_destinations
                        cities = scraper.scrape_all_destinations()
                        scraper.print_destination_report(cities)
                
            elif choice == '5':
                print("ð Thanks for using the Travel Destination Scraper!")
                break
                
            else:
                print("â Invalid choice. Please try again.")
                
        except KeyboardInterrupt:
            print("\nð Operation cancelled by user")
        except Exception as e:
            logger.error(f"Error: {e}")
            print(f"â An error occurred: {e}")

if __name__ == "__main__":
    main()  
